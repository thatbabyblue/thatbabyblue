<!-- Centered banner/logo (optional) -->
<p align="center">
  <img src="assets/logo.png" alt="Logo" width="180">
</p>

<h1 align="center">Hi, I'm Sean ğŸ‘‹</h1>
<p align="center">
  Deep Learning & LLMs Enthusiast
</p>

---

## âš“ï¸ What Iâ€™m About

Who would have thought some dabbling with LLMs, deep learning stuff has become some ongoing passion! I build and refine **LLM-powered systems**â€”from training tiny character-level GPTs to wiring up **inference pipelines**, **fine-tuning**, and **tool use/agents** for real apps. I care about:
- **Solid engineering** (profiling, testing, reproducibility)
- **Model quality** (data curation, evals, guardrails)

---

## ğŸ§  Deep Learning & LLMs

- **Training/Fine-Tuning**: GPT-style Transformers, LoRA/PEFT, efficient training on consumer GPUs
- **Inference/Serving**: vLLM, TGI, TensorRT-LLM, quantization (INT8/4), KV-cache tuning
- **Data & Evals**: dataset prep, prompt templating, hallucination checks, automatic eval harnesses
- **Tooling**: PyTorch, Hugging Face, Triton kernels (intro), ONNX, Weights & Biases

**Recent highlights**
- Built a **MiniGPT** (char-level) from scratch in PyTorch, trained on Shakespeare then fine-tuned on modern English (sampling, top-k/top-p, temperature).
- Implemented a **clean training loop** with gradient clipping, eval cycles, and checkpointing.
- Added **generate.py** with stable inference and safety knobs (temperature/top-k).

---

## ğŸ› ï¸ Developer Toolkit

**Languages**
- Python â€¢ Java â€¢ C++ â€¢ TypeScript/JavaScript â€¢ Bash

**Frameworks/Libs**
- PyTorch â€¢ Transformers â€¢ FastAPI â€¢ Spring Boot â€¢ gRPC â€¢ React

**Systems/DevOps**
- Docker â€¢ GitHub Actions â€¢ Linux â€¢ Make â€¢ Profilers (nvtop, nsight, perf)

**Data/Storage**
- SQLite/Postgres â€¢ Redis â€¢ Parquet/Arrow â€¢ Pandas/Polars

---

## ğŸ§ª Featured Projects

### ğŸ”¹ MiniGPT â€“ Character-Level Transformer
Train-from-scratch GPT with multi-head causal attention, GELU MLPs, layernorm, and sampling.
- **Tech**: PyTorch, Transformers
- **Highlights**: clean generation API, top-k, temperature, gradient clipping
- Repo: `thatbabyblue/GPTmini`

### ğŸ”¹ LLM Fine-Tuning Starter (LoRA/PEFT)
Template for supervised fine-tuning + eval harness on modest GPUs.
- **Tech**: PyTorch, PEFT, HF Datasets
- **Highlights**: mixed precision, checkpoint resume, eval dashboards

### ğŸ”¹ Java/C++ Systems Utilities
A grab-bag of performance-minded utilities, CI templates, and benchmarks.

> Want a polished README for any of these repos? Ping meâ€”Iâ€™ll ship one fast.

---

## ğŸ“ˆ Stats (because why not)

<p align="left">
  <img height="160" src="https://github-readme-stats.vercel.app/api?username=thatbabyblue&show_icons=true&hide_title=true&count_private=true&hide=prs&include_all_commits=true" alt="GitHub Stats">
  <img height="160" src="https://github-readme-stats.vercel.app/api/top-langs/?username=thatbabyblue&layout=compact&hide_title=true&langs_count=8" alt="Top Langs">
</p>

---

## ğŸ—ºï¸ What Iâ€™m Exploring Next

- **RAG systems** with robust retrieval & evals (colBERT, rerankers)
- **Agent frameworks** (tool use, planning), reliability in prod
- **Quantization** trade-offs: speed vs quality at different context lengths

---

## ğŸ“« Get in Touch

- GitHub: <a href="https://github.com/thatbabyblue">@thatbabyblue</a>  
- (Add your LinkedIn/Twitter/Email here)

---


<sub>Built with â¤ï¸ for deep learning, clean code, and shipping useful things.</sub>

