<!-- Centered banner/logo (optional) -->
<p align="center">
  <img src="assets/logo.png" alt="Logo" width="180">
</p>

<h1 align="center">Hi, I'm Sean 👋</h1>
<p align="center">
  Deep Learning & LLMs Enthusiast
</p>

---

## ⚓️ What I’m About

Who would have thought some dabbling with LLMs, deep learning stuff has become some ongoing passion! I build and refine **LLM-powered systems**—from training tiny character-level GPTs to wiring up **inference pipelines**, **fine-tuning**, and **tool use/agents** for real apps. I care about:
- **Solid engineering** (profiling, testing, reproducibility)
- **Model quality** (data curation, evals, guardrails)

---

## 🧠 Deep Learning & LLMs

- **Training/Fine-Tuning**: GPT-style Transformers, LoRA/PEFT, efficient training on consumer GPUs
- **Inference/Serving**: vLLM, TGI, TensorRT-LLM, quantization (INT8/4), KV-cache tuning
- **Data & Evals**: dataset prep, prompt templating, hallucination checks, automatic eval harnesses
- **Tooling**: PyTorch, Hugging Face, Triton kernels (intro), ONNX, Weights & Biases

**Recent highlights**
- Built a **MiniGPT** (char-level) from scratch in PyTorch, trained on Shakespeare then fine-tuned on modern English (sampling, top-k/top-p, temperature).
- Implemented a **clean training loop** with gradient clipping, eval cycles, and checkpointing.
- Added **generate.py** with stable inference and safety knobs (temperature/top-k).

---

## 🛠️ Developer Toolkit

**Languages**
- Python • Java • C++ • TypeScript/JavaScript • Bash

**Frameworks/Libs**
- PyTorch • Transformers • FastAPI • Spring Boot • gRPC • React

**Systems/DevOps**
- Docker • GitHub Actions • Linux • Make • Profilers (nvtop, nsight, perf)

**Data/Storage**
- SQLite/Postgres • Redis • Parquet/Arrow • Pandas/Polars

---

## 🧪 Featured Projects

### 🔹 MiniGPT – Character-Level Transformer
Train-from-scratch GPT with multi-head causal attention, GELU MLPs, layernorm, and sampling.
- **Tech**: PyTorch, Transformers
- **Highlights**: clean generation API, top-k, temperature, gradient clipping
- Repo: `thatbabyblue/GPTmini`

### 🔹 LLM Fine-Tuning Starter (LoRA/PEFT)
Template for supervised fine-tuning + eval harness on modest GPUs.
- **Tech**: PyTorch, PEFT, HF Datasets
- **Highlights**: mixed precision, checkpoint resume, eval dashboards

### 🔹 Java/C++ Systems Utilities
A grab-bag of performance-minded utilities, CI templates, and benchmarks.

> Want a polished README for any of these repos? Ping me—I’ll ship one fast.

---

## 📈 Stats (because why not)

<p align="left">
  <img height="160" src="https://github-readme-stats.vercel.app/api?username=thatbabyblue&show_icons=true&hide_title=true&count_private=true&hide=prs&include_all_commits=true" alt="GitHub Stats">
  <img height="160" src="https://github-readme-stats.vercel.app/api/top-langs/?username=thatbabyblue&layout=compact&hide_title=true&langs_count=8" alt="Top Langs">
</p>

---

## 🗺️ What I’m Exploring Next

- **RAG systems** with robust retrieval & evals (colBERT, rerankers)
- **Agent frameworks** (tool use, planning), reliability in prod
- **Quantization** trade-offs: speed vs quality at different context lengths

---

## 📫 Get in Touch

- GitHub: <a href="https://github.com/thatbabyblue">@thatbabyblue</a>  
- (Add your LinkedIn/Twitter/Email here)

---


<sub>Built with ❤️ for deep learning, clean code, and shipping useful things.</sub>

